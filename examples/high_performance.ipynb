{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EpiRust High-Performance Computing Demo\n",
    "\n",
    "This notebook demonstrates EpiRust's high-performance computing capabilities:\n",
    "\n",
    "1. Parallel processing with Rayon\n",
    "2. SIMD optimizations\n",
    "3. Memory optimization techniques\n",
    "4. NUMA-aware computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import perf_counter\n",
    "from epirust.parallel import ParallelExecutor\n",
    "from epirust.compute.simd import SimdOperations\n",
    "from epirust.memory import MemoryOptimizer\n",
    "\n",
    "# Set random seed and plotting style\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parallel Processing with Rayon\n",
    "\n",
    "Let's compare parallel vs sequential processing for large-scale computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def benchmark_parallel_processing(sizes):\n",
    "    executor = ParallelExecutor()\n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        # Generate random data\n",
    "        data = np.random.random((size, 100))\n",
    "        \n",
    "        # Sequential execution\n",
    "        start = perf_counter()\n",
    "        seq_result = executor.process_sequential(data)\n",
    "        seq_time = perf_counter() - start\n",
    "        \n",
    "        # Parallel execution\n",
    "        start = perf_counter()\n",
    "        par_result = executor.process_parallel(data)\n",
    "        par_time = perf_counter() - start\n",
    "        \n",
    "        results.append({\n",
    "            'size': size,\n",
    "            'sequential_time': seq_time,\n",
    "            'parallel_time': par_time,\n",
    "            'speedup': seq_time / par_time\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run benchmarks\n",
    "sizes = [1000, 10000, 100000, 1000000]\n",
    "parallel_results = benchmark_parallel_processing(sizes)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.loglog(parallel_results['size'], parallel_results['sequential_time'],\n",
    "         marker='o', label='Sequential')\n",
    "plt.loglog(parallel_results['size'], parallel_results['parallel_time'],\n",
    "         marker='s', label='Parallel')\n",
    "plt.xlabel('Data Size')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Processing Time vs Data Size')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogx(parallel_results['size'], parallel_results['speedup'],\n",
    "           marker='o')\n",
    "plt.xlabel('Data Size')\n",
    "plt.ylabel('Speedup Factor')\n",
    "plt.title('Parallel Processing Speedup')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SIMD Optimizations\n",
    "\n",
    "Compare performance across different SIMD instruction sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def benchmark_simd_operations(sizes):\n",
    "    simd_ops = SimdOperations()\n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        data = np.random.random(size)\n",
    "        \n",
    "        # Measure times for different implementations\n",
    "        times = {}\n",
    "        for impl in ['scalar', 'sse2', 'avx2', 'avx512']:\n",
    "            if simd_ops.has_capability(impl):\n",
    "                start = perf_counter()\n",
    "                _ = simd_ops.vector_sum(data, impl)\n",
    "                times[impl] = perf_counter() - start\n",
    "        \n",
    "        results.append({'size': size, **times})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run SIMD benchmarks\n",
    "sizes = [1000, 10000, 100000, 1000000]\n",
    "simd_results = benchmark_simd_operations(sizes)\n",
    "\n",
    "# Plot SIMD performance comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "for col in simd_results.columns:\n",
    "    if col != 'size':\n",
    "        plt.loglog(simd_results['size'], simd_results[col],\n",
    "                 marker='o', label=col.upper())\n",
    "\n",
    "plt.xlabel('Vector Size')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('SIMD Implementation Performance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory Optimization\n",
    "\n",
    "Demonstrate memory-efficient processing techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def benchmark_memory_optimization(sizes):\n",
    "    optimizer = MemoryOptimizer()\n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        # Generate test data\n",
    "        data = np.random.random((size, 50))\n",
    "        \n",
    "        # Standard processing\n",
    "        start = perf_counter()\n",
    "        std_result = optimizer.process_standard(data)\n",
    "        std_time = perf_counter() - start\n",
    "        std_mem = optimizer.measure_memory_usage()\n",
    "        \n",
    "        # Optimized processing\n",
    "        start = perf_counter()\n",
    "        opt_result = optimizer.process_optimized(data)\n",
    "        opt_time = perf_counter() - start\n",
    "        opt_mem = optimizer.measure_memory_usage()\n",
    "        \n",
    "        results.append({\n",
    "            'size': size,\n",
    "            'standard_time': std_time,\n",
    "            'optimized_time': opt_time,\n",
    "            'standard_memory': std_mem,\n",
    "            'optimized_memory': opt_mem\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run memory optimization benchmarks\n",
    "sizes = [1000, 10000, 100000]\n",
    "memory_results = benchmark_memory_optimization(sizes)\n",
    "\n",
    "# Plot memory usage and performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Memory usage\n",
    "ax1.loglog(memory_results['size'], memory_results['standard_memory'],\n",
    "         marker='o', label='Standard')\n",
    "ax1.loglog(memory_results['size'], memory_results['optimized_memory'],\n",
    "         marker='s', label='Optimized')\n",
    "ax1.set_xlabel('Data Size')\n",
    "ax1.set_ylabel('Memory Usage (MB)')\n",
    "ax1.set_title('Memory Usage Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Processing time\n",
    "ax2.loglog(memory_results['size'], memory_results['standard_time'],\n",
    "         marker='o', label='Standard')\n",
    "ax2.loglog(memory_results['size'], memory_results['optimized_time'],\n",
    "         marker='s', label='Optimized')\n",
    "ax2.set_xlabel('Data Size')\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.set_title('Processing Time Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NUMA-Aware Processing\n",
    "\n",
    "Demonstrate NUMA-aware computations for multi-socket systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def benchmark_numa_processing(sizes):\n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        data = np.random.random((size, 100))\n",
    "        \n",
    "        # Standard parallel processing\n",
    "        start = perf_counter()\n",
    "        _ = ParallelExecutor.process_standard(data)\n",
    "        std_time = perf_counter() - start\n",
    "        \n",
    "        # NUMA-aware processing\n",
    "        start = perf_counter()\n",
    "        _ = ParallelExecutor.process_numa_aware(data)\n",
    "        numa_time = perf_counter() - start\n",
    "        \n",
    "        results.append({\n",
    "            'size': size,\n",
    "            'standard_time': std_time,\n",
    "            'numa_time': numa_time,\n",
    "            'speedup': std_time / numa_time\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run NUMA benchmarks if system supports it\n",
    "if ParallelExecutor.has_numa_support():\n",
    "    sizes = [10000, 100000, 1000000]\n",
    "    numa_results = benchmark_numa_processing(sizes)\n",
    "    \n",
    "    # Plot NUMA performance comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogx(numa_results['size'], numa_results['speedup'],\n",
    "               marker='o')\n",
    "    plt.xlabel('Data Size')\n",
    "    plt.ylabel('Speedup Factor')\n",
    "    plt.title('NUMA-Aware Processing Speedup')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"NUMA support not detected on this system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary\n",
    "\n",
    "Let's summarize the performance gains from different optimizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Collect maximum speedups\n",
    "summary = {\n",
    "    'Parallel Processing': parallel_results['speedup'].max(),\n",
    "    'SIMD (vs Scalar)': simd_results.iloc[-1]['avx512'] / \n",
    "                        simd_results.iloc[-1]['scalar'] \n",
    "                        if 'avx512' in simd_results.columns else 'N/A',\n",
    "    'Memory Optimization': (memory_results['standard_memory'] / \n",
    "                          memory_results['optimized_memory']).max(),\n",
    "    'NUMA-Aware': numa_results['speedup'].max() if 'numa_results' in locals() else 'N/A'\n",
    "}\n",
    "\n",
    "print(\"Maximum Performance Improvements:\")\n",
    "for technique, speedup in summary.items():\n",
    "    if speedup != 'N/A':\n",
    "        print(f\"{technique}: {speedup:.2f}x\")\n",
    "    else:\n",
    "        print(f\"{technique}: Not Available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated EpiRust's high-performance computing capabilities:\n",
    "\n",
    "1. Parallel processing with Rayon provides significant speedup for large datasets\n",
    "2. SIMD optimizations offer additional performance gains for vector operations\n",
    "3. Memory optimization techniques reduce memory usage while maintaining performance\n",
    "4. NUMA-aware processing improves performance on multi-socket systems\n",
    "\n",
    "Key takeaways:\n",
    "- Performance gains scale with data size\n",
    "- Different optimization techniques can be combined\n",
    "- Hardware-specific optimizations (SIMD, NUMA) provide additional benefits when available"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}