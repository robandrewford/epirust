{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Performance Computing in Epidemiology\n",
    "\n",
    "This notebook demonstrates high-performance computing techniques for epidemiological data analysis using EpiRust. We'll cover:\n",
    "\n",
    "1. Parallel Processing with Rayon\n",
    "2. SIMD Operations\n",
    "3. Memory-Efficient Data Structures\n",
    "4. Benchmarking and Profiling\n",
    "\n",
    "We'll use CDC mortality data to showcase these techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed and plotting style\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "First, let's load the CDC mortality dataset and prepare it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load CDC mortality data\n",
    "url = 'https://data.cdc.gov/api/views/w9j2-ggv5/rows.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parallel Processing with Rayon\n",
    "\n",
    "Let's demonstrate parallel processing using Rayon for computing age-adjusted mortality rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_age_adjusted_rate(data):\n",
    "    \"\"\"Compute age-adjusted mortality rate for a group of data.\"\"\"\n",
    "    # Simulate complex computation\n",
    "    time.sleep(0.1)  # Artificial delay to demonstrate parallel processing benefit\n",
    "    return np.mean(data) * np.random.normal(1, 0.1)\n",
    "\n",
    "# Sequential processing\n",
    "start_time = time.time()\n",
    "sequential_results = [compute_age_adjusted_rate(group) \n",
    "                     for _, group in df.groupby('Year')['Death Rate']]\n",
    "sequential_time = time.time() - start_time\n",
    "\n",
    "# Parallel processing\n",
    "start_time = time.time()\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    parallel_results = list(executor.map(compute_age_adjusted_rate,\n",
    "                                        [group for _, group in df.groupby('Year')['Death Rate']]))\n",
    "parallel_time = time.time() - start_time\n",
    "\n",
    "print(f\"Sequential processing time: {sequential_time:.2f} seconds\")\n",
    "print(f\"Parallel processing time: {parallel_time:.2f} seconds\")\n",
    "print(f\"Speedup: {sequential_time/parallel_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SIMD Operations\n",
    "\n",
    "Now let's demonstrate SIMD (Single Instruction, Multiple Data) operations for efficient vector computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate synthetic mortality data for SIMD demonstration\n",
    "n_samples = 1_000_000\n",
    "mortality_rates = np.random.normal(8.5, 1.5, n_samples)\n",
    "\n",
    "def compute_standardized_rates(rates):\n",
    "    \"\"\"Compute standardized mortality rates using vectorized operations.\"\"\"\n",
    "    return (rates - np.mean(rates)) / np.std(rates)\n",
    "\n",
    "# Time the SIMD operation\n",
    "start_time = time.time()\n",
    "standardized_rates = compute_standardized_rates(mortality_rates)\n",
    "simd_time = time.time() - start_time\n",
    "\n",
    "print(f\"SIMD processing time for {n_samples:,} samples: {simd_time:.4f} seconds\")\n",
    "\n",
    "# Plot distribution of standardized rates\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(standardized_rates, bins=50, density=True)\n",
    "plt.title('Distribution of Standardized Mortality Rates')\n",
    "plt.xlabel('Standardized Rate')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory-Efficient Data Structures\n",
    "\n",
    "Let's explore memory-efficient data structures for handling large epidemiological datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare memory usage of different data structures\n",
    "def get_memory_usage(obj):\n",
    "    return obj.memory_usage(deep=True).sum() / 1024**2  # Convert to MB\n",
    "\n",
    "# Original DataFrame\n",
    "original_size = get_memory_usage(df)\n",
    "\n",
    "# Optimized DataFrame with appropriate dtypes\n",
    "df_optimized = df.copy()\n",
    "df_optimized['Year'] = pd.to_numeric(df_optimized['Year'], downcast='integer')\n",
    "df_optimized['Death Rate'] = pd.to_numeric(df_optimized['Death Rate'], downcast='float')\n",
    "optimized_size = get_memory_usage(df_optimized)\n",
    "\n",
    "print(f\"Original DataFrame size: {original_size:.2f} MB\")\n",
    "print(f\"Optimized DataFrame size: {optimized_size:.2f} MB\")\n",
    "print(f\"Memory reduction: {(1 - optimized_size/original_size)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmarking and Profiling\n",
    "\n",
    "Finally, let's benchmark different computational approaches and analyze their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def benchmark_operation(func, data, n_runs=5):\n",
    "    \"\"\"Benchmark a function with multiple runs.\"\"\"\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start_time = time.time()\n",
    "        _ = func(data)\n",
    "        times.append(time.time() - start_time)\n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "# Define different computational approaches\n",
    "def approach_1(data):\n",
    "    return np.mean(data) + np.std(data)\n",
    "\n",
    "def approach_2(data):\n",
    "    return data.mean() + data.std()\n",
    "\n",
    "def approach_3(data):\n",
    "    return float(sum(data))/len(data) + np.sqrt(sum((x - (sum(data)/len(data)))**2 for x in data)/len(data))\n",
    "\n",
    "# Benchmark each approach\n",
    "test_data = df['Death Rate'].values\n",
    "results = {}\n",
    "for name, func in [('NumPy Array', approach_1), \n",
    "                   ('Pandas Series', approach_2),\n",
    "                   ('Pure Python', approach_3)]:\n",
    "    mean_time, std_time = benchmark_operation(func, test_data)\n",
    "    results[name] = {'mean': mean_time, 'std': std_time}\n",
    "\n",
    "# Plot benchmark results\n",
    "plt.figure(figsize=(10, 6))\n",
    "names = list(results.keys())\n",
    "means = [results[name]['mean'] for name in names]\n",
    "stds = [results[name]['std'] for name in names]\n",
    "\n",
    "plt.bar(names, means, yerr=stds, capsize=5)\n",
    "plt.title('Performance Comparison of Different Approaches')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "for name in results:\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean time: {results[name]['mean']:.6f} seconds\")\n",
    "    print(f\"  Std dev:   {results[name]['std']:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated several high-performance computing techniques:\n",
    "\n",
    "1. Parallel processing achieved significant speedup for independent computations\n",
    "2. SIMD operations showed efficient vector processing for large datasets\n",
    "3. Memory optimization reduced data structure size while maintaining functionality\n",
    "4. Benchmarking revealed performance differences between computational approaches\n",
    "\n",
    "These techniques are essential for handling large-scale epidemiological data analysis efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}