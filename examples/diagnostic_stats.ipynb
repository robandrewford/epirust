{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EpiRust Diagnostic Statistics Demo\n",
    "\n",
    "This notebook demonstrates EpiRust's diagnostic statistics capabilities, including:\n",
    "\n",
    "1. Sensitivity and Specificity Analysis\n",
    "2. ROC Curve Generation\n",
    "3. Predictive Values\n",
    "4. Likelihood Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from epirust.stats import DiagnosticStats\n",
    "\n",
    "# Set random seed and plotting style\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Diagnostic Test Data\n",
    "\n",
    "Let's simulate a diagnostic test scenario with known disease status and test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_test_data(n_samples=1000, sensitivity=0.85, specificity=0.95, prevalence=0.1):\n",
    "    # True disease status\n",
    "    true_status = np.random.binomial(1, prevalence, n_samples)\n",
    "    \n",
    "    # Generate test results with given sensitivity and specificity\n",
    "    test_results = np.zeros_like(true_status)\n",
    "    \n",
    "    # True positives\n",
    "    diseased = true_status == 1\n",
    "    test_results[diseased] = np.random.binomial(1, sensitivity, np.sum(diseased))\n",
    "    \n",
    "    # False positives\n",
    "    healthy = true_status == 0\n",
    "    test_results[healthy] = np.random.binomial(1, 1 - specificity, np.sum(healthy))\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'true_status': true_status,\n",
    "        'test_result': test_results\n",
    "    })\n",
    "\n",
    "# Generate data\n",
    "df = generate_test_data()\n",
    "stats = DiagnosticStats(df['true_status'], df['test_result'])\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(stats.confusion_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Diagnostic Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate basic measures\n",
    "measures = {\n",
    "    'Sensitivity': stats.sensitivity(),\n",
    "    'Specificity': stats.specificity(),\n",
    "    'PPV': stats.positive_predictive_value(),\n",
    "    'NPV': stats.negative_predictive_value(),\n",
    "    'Accuracy': stats.accuracy(),\n",
    "    'F1 Score': stats.f1_score()\n",
    "}\n",
    "\n",
    "# Display results with confidence intervals\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Measure': name,\n",
    "        'Value': value,\n",
    "        'CI': stats.confidence_interval(name.lower())\n",
    "    }\n",
    "    for name, value in measures.items()\n",
    "])\n",
    "\n",
    "print(\"Diagnostic Measures with 95% Confidence Intervals:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood Ratios and Diagnostic Odds Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate likelihood ratios\n",
    "lr_positive = stats.positive_likelihood_ratio()\n",
    "lr_negative = stats.negative_likelihood_ratio()\n",
    "dor = stats.diagnostic_odds_ratio()\n",
    "\n",
    "print(f\"Positive Likelihood Ratio: {lr_positive:.2f}\")\n",
    "print(f\"Negative Likelihood Ratio: {lr_negative:.2f}\")\n",
    "print(f\"Diagnostic Odds Ratio: {dor:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve Analysis\n",
    "\n",
    "Let's generate and analyze the ROC curve for our test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate ROC curve data\n",
    "roc_data = stats.roc_curve()\n",
    "auc = stats.auc()\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(roc_data['fpr'], roc_data['tpr'], label=f'ROC (AUC = {auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prevalence Effects\n",
    "\n",
    "Let's examine how predictive values change with disease prevalence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate predictive values across different prevalences\n",
    "prevalences = np.linspace(0.01, 0.5, 50)\n",
    "ppvs = [stats.positive_predictive_value(prev) for prev in prevalences]\n",
    "npvs = [stats.negative_predictive_value(prev) for prev in prevalences]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(prevalences, ppvs, label='PPV')\n",
    "plt.plot(prevalences, npvs, label='NPV')\n",
    "plt.xlabel('Disease Prevalence')\n",
    "plt.ylabel('Predictive Value')\n",
    "plt.title('Predictive Values vs Disease Prevalence')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Analysis\n",
    "\n",
    "Let's perform bootstrap analysis to get robust confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform bootstrap analysis\n",
    "bootstrap_results = stats.bootstrap_analysis(n_iterations=1000)\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "measures = ['sensitivity', 'specificity', 'ppv', 'npv']\n",
    "titles = ['Sensitivity', 'Specificity', 'PPV', 'NPV']\n",
    "\n",
    "for ax, measure, title in zip(axes.flat, measures, titles):\n",
    "    sns.histplot(bootstrap_results[measure], ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Value')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display bootstrap confidence intervals\n",
    "print(\"\\nBootstrap 95% Confidence Intervals:\")\n",
    "for measure in measures:\n",
    "    ci = np.percentile(bootstrap_results[measure], [2.5, 97.5])\n",
    "    print(f\"{measure.upper()}: ({ci[0]:.3f}, {ci[1]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated EpiRust's comprehensive diagnostic statistics capabilities:\n",
    "\n",
    "1. Basic measures (sensitivity, specificity, predictive values)\n",
    "2. Advanced metrics (likelihood ratios, diagnostic odds ratio)\n",
    "3. ROC curve analysis\n",
    "4. Prevalence effects on predictive values\n",
    "5. Bootstrap analysis for confidence intervals\n",
    "\n",
    "These tools provide a robust framework for evaluating diagnostic tests in epidemiological studies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}